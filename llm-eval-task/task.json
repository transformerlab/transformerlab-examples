{
  "title": "LLM Evaluation (EleutherAI)",
  "name": "llm-eval-task",
  "command": "cd ~/llm-eval-task && python evaluate.py",
  "cpus": "4",
  "memory": "16",
  "accelerators": "RTX3090:1",
  "setup": "uv pip install lm-eval torch transformers accelerate transformerlab",
  "env_vars": {
    "HF_TOKEN": "ENTER YOUR HF_TOKEN HERE",
    "MODEL_NAME": "Qwen/Qwen2.5-0.5B",
    "EVAL_TASKS": "hellaswag,arc_easy",
    "BATCH_SIZE": "auto"
  },
  "description": "Evaluates an LLM using the EleutherAI LM Evaluation Harness. Defaults to Qwen2.5-0.5B on HellaSwag and ARC-Easy benchmarks."
}