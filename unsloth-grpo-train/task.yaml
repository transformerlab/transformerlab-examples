name: unsloth-grpo-train-task
github_repo_url: https://github.com/transformerlab/transformerlab-examples
github_repo_dir: unsloth-grpo-train
resources:
  accelerators: "A100-80GB:1"
setup: "uv pip install trl bitsandbytes unsloth transformerlab datasets jinja2"
run: "cd ~/unsloth-grpo-train && python train.py"
envs:
  WANDB_PROJECT: "unsloth-grpo-training"
  PYTHONUNBUFFERED: "1"
  HF_TOKEN: "{{secret._HF_TOKEN}}"
  WANDB_API_KEY: "{{secret._WANDB_API_KEY}}"
parameters:
  model_name: "unsloth/SmolLM2-135M"
  dataset: "openai/gsm8k"
  dataset_config: "main"
  dataset_split: "train"
  dataset_input_field: "question"
  dataset_output_field: "answer"
  start_thinking_string: "<reasoning>"
  end_thinking_string: "</reasoning>"
  start_answer_string: "<answer>"
  end_answer_string: "</answer>"
  lora_alpha: 32
  lora_dropout: 0.05
  lora_r: 16
  maximum_sequence_length: 1024
  maximum_completion_length: 512
  max_grad_norm: 0.3
  learning_rate: 0.00005
  learning_rate_schedule: "constant"
  batch_size: 1
  num_train_epochs: 1
  weight_decay: 0.0
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 0.00000001
  max_steps: 5
  system_prompt: "You are a helpful assistant that solves math problems step by step."
  input_template: "{{ question }}"
  output_template: "{{ answer }}"