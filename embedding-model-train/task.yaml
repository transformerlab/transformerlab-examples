name: embedding-model-train-task
github_repo_url: https://github.com/transformerlab/transformerlab-examples
github_repo_dir: embedding-model-train
resources:
  accelerators: "RTX3090:1"
setup: "uv pip install transformerlab numpy==1.26.4 pyarrow==15.0.0 fsspec==2023.9.0 huggingface-hub==0.36.0 transformers==4.57.3 datasets==2.16.0 sentence-transformers==5.2.0 torch==2.9.1 accelerate==1.12.0"
run: "cd ~/embedding-model-train && python train.py"
envs:
  WANDB_PROJECT: "embedding-model-training"
  WANDB_API_KEY: "ENTER YOUR WANDB_API_KEY HERE"
  PYTHONUNBUFFERED: "1"
  HF_TOKEN: "ENTER YOUR HF_TOKEN HERE"
parameters:
  model_name: "sentence-transformers/paraphrase-MiniLM-L3-v2"
  dataset_name: "sentence-transformers/stsb"
  dataset_type: "anchor | positive"
  loss_function: "MultipleNegativesRankingLoss"
  num_train_epochs: "1"
  max_steps: "5"
  max_samples: "100"
  batch_size: "8"
  learning_rate: "2e-5"