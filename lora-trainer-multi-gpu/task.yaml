name: lora-trainer-multi-gpu-task
github_repo_url: https://github.com/transformerlab/transformerlab-examples
github_repo_dir: lora-trainer-multi-gpu
resources:
  accelerators: "RTX3090:2"
setup: "uv pip install transformers trl peft bitsandbytes accelerate transformerlab datasets wandb"
run: "cd ~/lora-trainer-multi-gpu && python train.py"
envs:
  WANDB_PROJECT: "lora-multi-gpu-training"
  WANDB_API_KEY: "ENTER YOUR WANDB_API_KEY HERE"
  PYTHONUNBUFFERED: "1"
  HF_TOKEN: "ENTER YOUR HF_TOKEN HERE"