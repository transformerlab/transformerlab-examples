name: mlx-lora-train-task
github_repo_url: https://github.com/transformerlab/transformerlab-examples
github_repo_dir: mlx_lora_train
resources:
  accelerators: "M1:1"
setup: "uv pip install transformerlab mlx==0.29.3 mlx-lm==0.28.3 datasets jinja2 transformers huggingface-hub pyyaml wandb"
run: "cd ~/mlx_lora_train && python train.py"
envs:
  PYTHONUNBUFFERED: "1"
  HF_TOKEN: "ENTER YOUR HF_TOKEN HERE"
  WANDB_API_KEY: "ENTER YOUR WANDB_API_KEY HERE"
parameters:
  model_name: "mlx-community/Llama-3.2-1B-Instruct-4bit"
  dataset: "Trelis/touch-rugby-rules"
  adaptor_name: "adaptor"
  lora_layers: 16
  lora_rank: 8
  lora_alpha: 16
  learning_rate: 0.00005
  batch_size: 4
  iters: 1000
  num_train_epochs: -1
  steps_per_report: 100
  steps_per_eval: 200
  save_every: 100
  fuse_model: true
  log_to_wandb: false
