{
  "title": "LLM-as-Judge Evaluation Task",
  "name": "llm-judge-eval-task",
  "command": "cd ~/llm-judge-eval && python train.py",
  "accelerators": "RTX3090:1",
  "setup": "uv pip install deepeval==2.8.2 langchain-openai==0.3.28 instructor==1.9.2 anthropic==0.49.0 seaborn==0.13.2 langchain==0.3.25 langchain-community==0.3.23 transformerlab datasets pandas",
  "env_vars": {
    "WANDB_PROJECT": "llm-judge-evaluation",
    "PYTHONUNBUFFERED": "1",
    "OPENAI_API_KEY": "<your_openai_api_key_here>",
    "ANTHROPIC_API_KEY": "<your_anthropic_api_key_here>"
  },
  "parameters": {
    "dataset": "your_dataset_name",
    "dataset_split": "train",
    "generation_model": "gpt-4o-mini",
    "predefined_tasks": "[\"Answer Relevancy\", \"Faithfulness\"]",
    "limit": "1.0",
    "threshold": "0.5"
  },
  "description": "Using LLMs as Judges for evaluating outputs of other LLMs. Powered by DeepEval Framework. Requires a dataset with 'input', 'output', and 'expected_output' columns."
}
