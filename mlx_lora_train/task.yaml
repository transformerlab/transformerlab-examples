name: mlx-lora-train-task
github_repo_url: https://github.com/transformerlab/transformerlab-examples
github_repo_dir: mlx_lora_train
resources:
  accelerators: "M-Series:1"
setup: "uv pip install transformerlab mlx==0.29.3 mlx-lm==0.28.3 datasets pyyaml"
run: "cd ~/mlx_lora_train && python train.py"
envs:
  WANDB_PROJECT: "mlx-lora-training"
  PYTHONUNBUFFERED: "1"
  HF_TOKEN: "ENTER YOUR HF_TOKEN HERE"
  WANDB_API_KEY: "ENTER YOUR WANDB_API_KEY HERE"
parameters:
  model_name: "mlx-community/Llama-3.2-1B-Instruct-4bit"
  dataset: "yahma/alpaca-cleaned"
  lora_layers: 16
  batch_size: 4
  learning_rate: 0.00005
  lora_rank: 8
  lora_alpha: 16
  iters: 1000
  num_train_epochs: -1
  steps_per_report: 100
  steps_per_eval: 200
  save_every: 100
  adaptor_name: "adaptor"
  fuse_model: true
  formatting_chat_template: ""
  chatml_formatted_column: "messages"
  formatting_template: ""
  log_to_wandb: true
